{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cmfUM4uKy_U9"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import random\n",
        "import sys\n",
        "from scipy.special import softmax\n",
        "from collections import namedtuple, deque\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Network hyperparameters\n",
        "# Type 1\n",
        "t1_units = [256,256,256]\n",
        "# Type 2\n",
        "t2_units = [64,64,64]\n",
        "\n",
        "value_units = 1\n",
        "adv_units = 2"
      ],
      "metadata": {
        "id": "-JscJN_3yb8N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dueling DQN Type 1\n",
        "\n",
        "class DuelDQNettype1(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(DuelDQNettype1, self).__init__()\n",
        "\n",
        "    self.dense1 = tf.keras.layers.Dense(t1_units[0], activation = tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(t1_units[1], activation = tf.nn.relu)\n",
        "    self.dense3 = tf.keras.layers.Dense(t1_units[2], activation = tf.nn.relu)\n",
        "    self.value_layer= tf.keras.layers.Dense(value_units)\n",
        "    self.advantage_layer = tf.keras.layers.Dense(adv_units)\n",
        "\n",
        "  def call(self,inp):\n",
        "\n",
        "    d1 = self.dense1(inp)\n",
        "    v1 = self.dense2(d1)\n",
        "    a1 = self.dense3(d1)\n",
        "\n",
        "    value = self.value_layer(v1)\n",
        "    advantage = self.advantage_layer(a1)\n",
        "\n",
        "    # Type 1\n",
        "    aggregate = tf.reduce_mean(advantage, 1, True)\n",
        "\n",
        "    Q = value + (advantage - aggregate)\n",
        "\n",
        "    return Q\n",
        "\n",
        "  def choose_action(self, state, num_actions, tau):\n",
        "    Q = self.call(tf.expand_dims(state, axis=0))\n",
        "    Q_vals = tf.reshape(Q, [num_actions,])\n",
        "    probs = softmax(Q_vals/tau)\n",
        "    act = np.random.choice(num_actions, 1, p=probs)[0]\n",
        "    return act\n"
      ],
      "metadata": {
        "id": "TDBvG6HJzC15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1082651-d9e1-47c3-d72e-6f627ee9acde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dueling DQN Type 2\n",
        "\n",
        "class DuelDQNettype2(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(DuelDQNettype2, self).__init__()\n",
        "\n",
        "    self.dense1 = tf.keras.layers.Dense(t2_units[0], activation = tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(t2_units[1], activation = tf.nn.relu)\n",
        "    self.dense3 = tf.keras.layers.Dense(t2_units[2], activation = tf.nn.relu)\n",
        "    self.value_layer= tf.keras.layers.Dense(value_units)\n",
        "    self.advantage_layer = tf.keras.layers.Dense(adv_units)\n",
        "\n",
        "  def call(self,inp):\n",
        "\n",
        "    d1 = self.dense1(inp)\n",
        "    v1 = self.dense2(d1)\n",
        "    a1 = self.dense3(d1)\n",
        "\n",
        "    value = self.value_layer(v1)\n",
        "    advantage = self.advantage_layer(a1)\n",
        "\n",
        "    # Type 2\n",
        "    aggregate = tf.math.reduce_max(advantage)\n",
        "    aggregate = tf.cast(tf.reshape(tf.argmax(advantage, 1), [len(advantage), 1]), tf.float64)\n",
        "\n",
        "    Q = value + (advantage - aggregate)\n",
        "\n",
        "    return Q\n",
        "\n",
        "  def choose_action(self, state, num_actions, tau):\n",
        "    Q = self.call(tf.expand_dims(state, axis=0))\n",
        "    Q_vals = tf.reshape(Q, [num_actions,])\n",
        "    probs = softmax(Q_vals/tau)\n",
        "    act = np.random.choice(num_actions, 1, p=probs)[0]\n",
        "    return act\n"
      ],
      "metadata": {
        "id": "Y7Z0k3LT2a5_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemoryBuffer():\n",
        "  def __init__(self,memory_size):\n",
        "    self.memory_size = memory_size\n",
        "    self.mem = deque(maxlen = memory_size)\n",
        "\n",
        "  def record(self, experience_tuple):\n",
        "    self.mem.append(experience_tuple)\n",
        "\n",
        "  def size(self):\n",
        "    return len(self.mem)\n",
        "\n",
        "  def sample(self,batch_size):\n",
        "    if batch_size > len(self.mem):\n",
        "      batch_size = len(self.mem)\n",
        "    idx = np.random.choice(np.arange(len(self.mem)),size=batch_size,replace=False)\n",
        "    batch_mem = [self.mem[i] for i in idx]\n",
        "    return batch_mem"
      ],
      "metadata": {
        "id": "e0HhoZ2M1NFO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DuelingDQN():\n",
        "    def __init__(self, type_ddqn):\n",
        "        # Creating environment\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "\n",
        "        # Environment parameters\n",
        "        self.num_actions = self.env.action_space.n\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "\n",
        "        # Initializing networks and optimizers based on Type 1 or Type\n",
        "        self.type = type_ddqn\n",
        "        if self.type == 1:\n",
        "            self.ddqn = DuelDQNettype1()\n",
        "            self.ddqn_tgt = DuelDQNettype1()\n",
        "        if self.type == 2:\n",
        "            self.ddqn = DuelDQNettype2()\n",
        "            self.ddqn_tgt = DuelDQNettype2()\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "        # Setting target network weights to learning network's weights\n",
        "        self.ddqn_tgt.set_weights(self.ddqn.get_weights())\n",
        "\n",
        "        self.memory = MemoryBuffer(mem_size)\n",
        "\n",
        "\n",
        "    def train_net(self, states, actions, rewards, next_states, dones):\n",
        "        # Optimizing dueling dqn learning network\n",
        "        next_act_l = tf.cast(tf.argmax(self.ddqn(next_states), 1), tf.int32)\n",
        "        next_act_l_idx = tf.stack([tf.range(tf.shape(next_act_l)[0]), next_act_l], 1)\n",
        "        q_tgt = self.ddqn_tgt(next_states)\n",
        "        Q_target = rewards + (1 - dones) * gamma * tf.stop_gradient(tf.gather_nd(q_tgt, next_act_l_idx))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            act_l = tf.cast(actions, tf.int32)\n",
        "            act_l_idx = tf.stack([tf.range(tf.shape(act_l)[0]), act_l], 1)\n",
        "            q_l = self.ddqn(states)\n",
        "            Q_pred = tf.gather_nd(q_l, act_l_idx)\n",
        "            loss = self.loss_fn(Q_pred, Q_target)\n",
        "\n",
        "        grad = tape.gradient(loss, self.ddqn.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grad, self.ddqn.trainable_variables))\n",
        "\n",
        "\n",
        "    def update_target(self):\n",
        "        self.ddqn_tgt.set_weights(self.ddqn.get_weights())\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        tf.keras.backend.set_floatx('float64')\n",
        "        running_reward = 10\n",
        "        avg_running_reward = []\n",
        "        episode_rewards_total = []\n",
        "\n",
        "        state = self.env.reset()\n",
        "        train_step = 0\n",
        "        tau = tau_initial\n",
        "\n",
        "        for ep in range(num_episodes):\n",
        "            ep_reward = 0\n",
        "\n",
        "            for iter in range(iterations):\n",
        "                # Softmax\n",
        "                action = self.ddqn.choose_action(state, self.num_actions, tau)\n",
        "                next_state,reward,done,_ = self.env.step(action) # sending action to environment\n",
        "                ep_reward += reward\n",
        "                # Recording experience\n",
        "                self.memory.record((state, action, reward, next_state, done))\n",
        "\n",
        "                if self.memory.size() > batch_size:\n",
        "                    # Training learning network\n",
        "                    train_step += 1\n",
        "                    if  train_step % update_tgtstep == 0:\n",
        "                        # Updating target network weights\n",
        "                        self.update_target()\n",
        "\n",
        "                    # Sampling experience\n",
        "                    batch = self.memory.sample(batch_size)\n",
        "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "                    # Computing loss, gradients and training learning network\n",
        "                    loss = self.train_net(np.asarray(states), np.asarray(actions), np.asarray(rewards), np.asarray(next_states), np.asarray(dones))\n",
        "\n",
        "                tau = max(tau_final, tau_decay*tau)\n",
        "                if done:\n",
        "                  break\n",
        "                state = next_state\n",
        "\n",
        "            # Calculating running reward\n",
        "            running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "            if running_reward > self.env.spec.reward_threshold:\n",
        "                print(f\"Solved! Running reward is now {ep_reward} and the last episode runs to {iter} time steps!\")\n",
        "                # break\n",
        "            if ep % 100 == 0:\n",
        "                print(f\"Episode {ep}: Reward = {ep_reward} | Avg Reward = {running_reward}\")\n",
        "            avg_running_reward.append(running_reward)\n",
        "            episode_rewards_total.append(ep_reward)\n",
        "\n",
        "        return avg_running_reward, episode_rewards_total"
      ],
      "metadata": {
        "id": "7r3CxxkRH97m"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.001\n",
        "num_episodes = 2\n",
        "iterations = 500\n",
        "update_tgtstep = 4\n",
        "mem_size = 50_000\n",
        "batch_size = 16\n",
        "\n",
        "# Softmax\n",
        "tau_decay = 0.995\n",
        "tau_initial = 0.8\n",
        "tau_final = 0.001"
      ],
      "metadata": {
        "id": "eH_Nx6ig4T0g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Dueling DQN models over 5 runs to account for stochasticity\n",
        "num_runs = 5"
      ],
      "metadata": {
        "id": "9KvUiI75g_Dh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_over_runs_t1 = []\n",
        "reward_over_runs_t2 = []\n",
        "for r in range(num_runs):\n",
        "    # Creating model of Type 1 eqn\n",
        "    type_ddqn = 1\n",
        "    model_t1 = DuelingDQN(type_ddqn)\n",
        "    # Training model of Type 1 eqn\n",
        "    rewards_t1, re_ep_t1 = model_t1.train()\n",
        "    reward_over_runs_t1.append(rewards_t1)\n",
        "\n",
        "    # Creating model of Type 2 eqn\n",
        "    type_ddqn = 2\n",
        "    model_t2 = DuelingDQN(type_ddqn)\n",
        "    # Training model of Type 2 eqn\n",
        "    rewards_t2, re_ep_t2 = model_t2.train()\n",
        "    reward_over_runs_t2.append(rewards_t2)"
      ],
      "metadata": {
        "id": "HwfJsMGj4i3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "returns_t1_arr = np.asarray(reward_over_runs_t1)\n",
        "returns_t2_arr = np.asarray(reward_over_runs_t2)\n",
        "\n",
        "# Computing mean over 5 runs across 500 episodes\n",
        "mean_return_t1 = np.mean(returns_t1_arr, axis=0)\n",
        "mean_return_t2 = np.mean(returns_t2_arr, axis=0)\n",
        "\n",
        "# Computing standard deviation over 5 runs across 500 episodes\n",
        "std_return_t1 = np.std(returns_t1_arr, axis=0)\n",
        "std_return_t2 = np.std(returns_t2_arr, axis=0)"
      ],
      "metadata": {
        "id": "CZGhzjco5RUy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison by plotting\n",
        "plt.figure()\n",
        "plt.plot(mean_return_t1, c='r', label='Type 1')\n",
        "plt.fill_between(np.arange(0,num_episodes), mean_return_t1+ std_return_t1, mean_return_t1 - std_return_t1, alpha=0.2)\n",
        "plt.plot(mean_return_t2, c='b', label='Type 2')\n",
        "plt.fill_between(np.arange(0,num_episodes), mean_return_t2+ std_return_t2, mean_return_t2 - std_return_t2, alpha=0.2)\n",
        "plt.xlabel('Episode Number')\n",
        "plt.ylabel('Episodic Return')\n",
        "plt.title(f'Dueling DQN (CartPole v1)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XCBO-IfJ5J5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
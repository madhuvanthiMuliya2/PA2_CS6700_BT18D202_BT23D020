{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L1XbCLAyzXRU"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Network hyperparameters\n",
        "policy_hidden = 64\n",
        "\n",
        "# for baseline\n",
        "value_hidden = 64\n",
        "value_out = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "watgNlFYey9Q",
        "outputId": "01f09eae-bc99-4023-d101-5e772cae7fb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Network\n",
        "class PolicyNet(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(policy_hidden, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(num_actions, activation='softmax')\n",
        "\n",
        "    def call(self, inp):\n",
        "        d1 = self.dense1(inp)\n",
        "        d2 = self.dense2(d1)\n",
        "        return d2"
      ],
      "metadata": {
        "id": "6B-CE6kZeps_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value Network for baseline\n",
        "class ValueNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(ValueNet, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(value_hidden, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(value_out)\n",
        "\n",
        "    def call(self, inp):\n",
        "        d1 = self.dense1(inp)\n",
        "        d2 = self.dense2(d1)\n",
        "        return d2"
      ],
      "metadata": {
        "id": "7K4UO771et9I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class REINFORCE():\n",
        "    def __init__(self, baseline):\n",
        "        # Creating environment\n",
        "        self.env = gym.make('Acrobot-v1')\n",
        "\n",
        "        # Environment parameters\n",
        "        self.num_actions = self.env.action_space.n\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "\n",
        "        # Initializing networks and optimizers based on baseline = True or False\n",
        "        self.baseline = baseline\n",
        "        if not self.baseline:\n",
        "            self.policy = PolicyNet(self.num_actions)\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "        else:\n",
        "            self.policy = PolicyNet(self.num_actions)\n",
        "            self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "            self.value_net = ValueNet()\n",
        "            self.val_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        # Choosing action using probabilities determined by policy network\n",
        "        state_re = tf.expand_dims(state, axis=0)\n",
        "        probs = self.policy(state_re).numpy()[0]\n",
        "        action = np.random.choice(len(probs), p=probs)\n",
        "        return action\n",
        "\n",
        "\n",
        "    def calc_discounted_rew(self, rewards):\n",
        "        # Computing discounted reward, Gt\n",
        "        rshape = len(rewards)\n",
        "        disc_rew = np.zeros((rshape,), dtype=np.float32)\n",
        "        Gt = 0\n",
        "        for r in reversed(range(len(rewards))):\n",
        "            Gt = Gt * gamma + rewards[r]\n",
        "            disc_rew[r] = Gt\n",
        "        return disc_rew\n",
        "\n",
        "    def train_policy_net(self, states, actions, disc_rew):\n",
        "        # Optimizing policy network without baseline\n",
        "        with tf.GradientTape() as tape:\n",
        "            act_probs = self.policy(states)\n",
        "            act_mask = tf.one_hot(actions, depth=act_probs.shape[1])\n",
        "            log_probs = tf.reduce_sum(act_mask * tf.math.log(act_probs), axis=1)\n",
        "            loss = -tf.reduce_mean(log_probs * disc_rew)\n",
        "\n",
        "        grad = tape.gradient(loss, self.policy.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grad, self.policy.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train_policy_net_baseline(self, states, actions, disc_rew):\n",
        "        # Optimizing policy network with baseline\n",
        "        with tf.GradientTape() as tape:\n",
        "            act_probs = self.policy(states)\n",
        "            act_mask = tf.one_hot(actions, depth=act_probs.shape[1])\n",
        "            log_probs = tf.reduce_sum(act_mask * tf.math.log(act_probs), axis=1)\n",
        "            adv = disc_rew - self.value_net(tf.convert_to_tensor(states, dtype=tf.float32))\n",
        "            loss = -tf.reduce_mean(log_probs * adv)\n",
        "\n",
        "        grad = tape.gradient(loss, self.policy.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grad, self.policy.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train_value_net(self, states, disc_rew):\n",
        "        # Optimizing value network to obtain baseline\n",
        "        with tf.GradientTape() as tape:\n",
        "            values = self.value_net(states)\n",
        "            loss = tf.reduce_mean(tf.square(disc_rew - tf.squeeze(values)))\n",
        "\n",
        "        grad = tape.gradient(loss, self.value_net.trainable_variables)\n",
        "        self.val_optimizer.apply_gradients(zip(grad, self.value_net.trainable_variables))\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        episode_rewards_total = []\n",
        "\n",
        "        for ep in range(num_episodes):\n",
        "            ep_states = []\n",
        "            ep_actions = []\n",
        "            ep_rewards = []\n",
        "\n",
        "            state = self.env.reset()\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                ep_states.append(state)\n",
        "                ep_actions.append(action)\n",
        "                ep_rewards.append(reward)\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            discounted_rewards = self.calc_discounted_rew(ep_rewards)\n",
        "            states = np.array(ep_states)\n",
        "            actions = np.array(ep_actions)\n",
        "            if self.baseline:\n",
        "                loss = self.train_policy_net_baseline(states, actions, discounted_rewards)\n",
        "                self.train_value_net(states, discounted_rewards)\n",
        "            if not self.baseline:\n",
        "                loss = self.train_policy_net(states, actions, discounted_rewards)\n",
        "\n",
        "\n",
        "            cumulative_reward = np.sum(ep_rewards)\n",
        "            episode_rewards_total.append(cumulative_reward)\n",
        "\n",
        "            print(f\"Episode {ep}: Cumulative Reward = {cumulative_reward}\")\n",
        "\n",
        "        self.env.close()\n",
        "        return episode_rewards_total\n",
        "\n"
      ],
      "metadata": {
        "id": "7r3CxxkRH97m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "gamma = 0.99\n",
        "learning_rate = 0.001\n",
        "num_episodes = 500"
      ],
      "metadata": {
        "id": "eH_Nx6ig4T0g"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training REINFORCE models over 5 runs to account for stochasticity\n",
        "num_runs = 5"
      ],
      "metadata": {
        "id": "9KvUiI75g_Dh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_over_runs_wob = []\n",
        "reward_over_runs_b = []\n",
        "for r in range(num_runs):\n",
        "    # Creating model without baseline\n",
        "    model_wob = REINFORCE(False)\n",
        "    # Training model without baseline\n",
        "    rewards_wob = model_wob.train()\n",
        "    reward_over_runs_wob.append(rewards_wob)\n",
        "\n",
        "    # Creating model with baseline\n",
        "    model_b = REINFORCE(True)\n",
        "    # Training model with baseline\n",
        "    rewards_b = model_b.train()\n",
        "    reward_over_runs_b.append(rewards_b)"
      ],
      "metadata": {
        "id": "HwfJsMGj4i3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "returns_wob_arr = np.asarray(reward_over_runs_wob)\n",
        "returns_b_arr = np.asarray(reward_over_runs_b)\n",
        "\n",
        "# Computing mean over 5 runs across 500 episodes\n",
        "mean_return_wob = np.mean(returns_wob_arr, axis=0)\n",
        "mean_return_ba = np.mean(returns_b_arr, axis=0)\n",
        "\n",
        "# Computing standard deviation over 5 runs across 500 episodes\n",
        "std_return_wob = np.std(returns_wob_arr, axis=0)\n",
        "std_return_ba = np.std(returns_b_arr, axis=0)"
      ],
      "metadata": {
        "id": "CZGhzjco5RUy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison by plotting\n",
        "plt.figure()\n",
        "plt.plot(mean_return_ba, c='r', label='REINFORCE_w_baseline')\n",
        "plt.fill_between(np.arange(0,num_episodes), mean_return_ba+ std_return_ba, mean_return_ba - std_return_ba, alpha=0.2)\n",
        "plt.plot(mean_return_wob, c='b', label='REINFORCE_wout_baseline')\n",
        "plt.fill_between(np.arange(0,num_episodes), mean_return_wob+ std_return_wob, mean_return_wob - std_return_wob, alpha=0.2)\n",
        "plt.xlabel('Episode Number')\n",
        "plt.ylabel('Episodic Return')\n",
        "plt.title(f'REINFORCE (Acrobot v1)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XCBO-IfJ5J5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}